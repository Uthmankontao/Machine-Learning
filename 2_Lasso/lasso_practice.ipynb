{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc5c99e",
   "metadata": {},
   "source": [
    "TP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2211e588",
   "metadata": {},
   "source": [
    "### 1. Génération des données du problème. <br>\n",
    "#### a) Générez les données du problème"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "24bebc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: [309 390  55  82 329]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxpy as cvx\n",
    "import time\n",
    "n = 200 # number of examples (you can try with n = 1000 and n = 5000)\n",
    "p = 2*n # dimensionality of the problem\n",
    "k = 5 # number of active variables\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(n,p) # creating features and normalizing them\n",
    "X = (X - np.mean(X,axis = 0))/np.std(X,axis = 0)\n",
    "t = np.arange(0,p)/(p-1); # bulding the variance matrix !\n",
    "S = np.zeros((p,p))\n",
    "nn = 0.00001\n",
    "for i in range(p):\n",
    "    S[i,:] = np.exp(-(t-t[i])**2/nn);\n",
    "X = X@(S**.5)\n",
    "X = X/np.linalg.norm(X,axis=0)\n",
    "ind = np.random.choice(p, k, replace=False) # generating optimal weights\n",
    "print(f\"index: {ind}\")\n",
    "weights = np.random.randn(k)\n",
    "weights += 0.1+np.sign(weights) # to get large enough weight\n",
    "wopt = np.zeros(p)\n",
    "wopt[ind] = weights\n",
    "rsnr = 2 # generating output by X@w + noise\n",
    "z = X[:,ind]@weights\n",
    "stdnoise = np.std(z)/rsnr\n",
    "y = z + stdnoise*np.random.randn(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd871cb",
   "metadata": {},
   "source": [
    "#### b) Vérifiez que les données ont bien les propriétés attendues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ca952c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de X: (200, 400)\n",
      "Taille de Y: (200,)\n",
      "Taille de S: (400, 400)\n",
      "weights: [-1.12180992  1.31787313 -0.92626818  2.65803182 -1.51223091]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Taille de X: {X.shape}\")\n",
    "print(f\"Taille de Y: {y.shape}\")\n",
    "print(f\"Taille de S: {S.shape}\")\n",
    "print(f\"weights: {weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ecaa275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[ 0.14467085  0.16433926  0.19901953 ...  0.14290542  0.1631462\n",
      "   0.13879548]\n",
      " [-0.05527668 -0.03926178 -0.01261321 ... -0.08845567 -0.09053221\n",
      "  -0.08149685]\n",
      " [ 0.1156909   0.09286922  0.05119743 ... -0.06945269 -0.09496954\n",
      "  -0.05753581]\n",
      " [-0.091624   -0.05659364  0.00736161 ... -0.00039534 -0.04531035\n",
      "  -0.06724717]\n",
      " [ 0.00719499 -0.03644546 -0.09217101 ...  0.00423391 -0.07740297\n",
      "  -0.11560728]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"X:\\n {X[:5,:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf569e89",
   "metadata": {},
   "source": [
    "#### c) Calculez l’erreur de généralisation \"in sample\" de la méthode des moindres carrés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f54845e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error for the LS regression: 2.7231\n"
     ]
    }
   ],
   "source": [
    "b_ls = np.linalg.solve(X.T@X,X.T@y)\n",
    "e_ls = np.sum((X@b_ls-z)**2)\n",
    "print(\"Test error for the LS regression: {:0.4f}\".format(e_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d22804",
   "metadata": {},
   "source": [
    "#### d) Écrire une fonction Eval_coef(X,z,coeff), qui calcule l’erreur de généralisation \"insample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f5b1297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error for the LS regression (utilisant mon Eval_coef function): 2.7231\n"
     ]
    }
   ],
   "source": [
    "def Eval_coef(X,z,coeff):\n",
    "    e = np.sum((X@coeff - z)**2)\n",
    "    return e\n",
    "print(f\"Test error for the LS regression (utilisant mon Eval_coef function): {Eval_coef(X,z,b_ls):0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bc3b06",
   "metadata": {},
   "source": [
    "### 2. Différentes manières de résoudre le problème du Lasso <br>\n",
    "#### a) Écrire un programme CVX résolvant, pour λ = 10−3n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aac57a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In sample error for the LS regression: 2.7231\n",
      "In sample error for the Lasso regression: 0.3323\n"
     ]
    }
   ],
   "source": [
    "λ = 1e-3*n # le paramètre de régularisation\n",
    "b = cvx.Variable(p) # le vecteur des coefficients, il sert de variable d'optimisation\n",
    "obj = cvx.Minimize(0.5*cvx.sum_squares(X@b - y) + λ*cvx.norm(b, 1)) # la fonction objectif à minimiser\n",
    "prob = cvx.Problem(obj) # le problème d'optimisation\n",
    "start_time = time.time()\n",
    "prob.solve(solver=cvx.SCS, eps=1e-5) # résolution du problème avec le solveur SCS\n",
    "end_time = time.time()\n",
    "\n",
    "# b) Vérifiez que la solution obtenue est meilleure que celle des moindres carrés\n",
    "b_ls = np.linalg.solve(X.T@X,X.T@y) # recalcul des coefficients de la régression des moindres carrés\n",
    "e_ls = np.sum((X@b_ls-z)**2) \n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format}) # pour afficher les nombres avec 3 décimales\n",
    "print(\"In sample error for the LS regression: {:0.4f}\".format(e_ls))\n",
    "\n",
    "e_la = np.sum((X@b.value-z)**2)\n",
    "print(\"In sample error for the Lasso regression: {:0.4f}\".format(e_la))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86e3ffa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alors la methode lasso est bien meilleure que les moindres carrés. avec une erreur de 2.3908 de moins\n",
      "Le temps d'execution est de 0.1795 secondes\n",
      "Le nombre de coefficients non nuls est de 24\n",
      "Les coefficients non nuls sont aux indices: [ 18  22  42  43  47  55  70  82  93 118 178 190 191 261 275 291 302 309\n",
      " 310 329 389 390 393 397]\n",
      "Les coefficients estimés sont: [-0.042  0.081 -0.003 -0.026 -0.018 -0.697 -0.002  2.623 -0.063  0.087\n",
      "  0.030 -0.192 -0.002 -0.023  0.045 -0.062  0.005 -0.683 -0.018 -1.442\n",
      "  0.008  1.074  0.036 -0.019]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Alors la methode lasso est bien meilleure que les moindres carrés. avec une erreur de {e_ls-e_la:0.4f} de moins\")\n",
    "print(f\"Le temps d'execution est de {end_time - start_time:0.4f} secondes\")\n",
    "print(f\"Le nombre de coefficients non nuls est de {np.sum(np.abs(b.value)>1e-4)}\")\n",
    "print(f\"Les coefficients non nuls sont aux indices: {np.where(np.abs(b.value)>1e-4)[0]}\")\n",
    "print(f\"Les coefficients estimés sont: {b.value[np.where(np.abs(b.value)>1e-4)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405689e1",
   "metadata": {},
   "source": [
    "#### b) Écrire un programme CVX résolvant la formulation suivant de Lasso, avec une valeur <br> de t permettant d’obtenir les mêmes résultats que le problème précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab65824e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In sample error for the Lasso regression (formulation contrainte): 0.3322\n"
     ]
    }
   ],
   "source": [
    "t = np.sum(np.abs(b.value))\n",
    "b1 = cvx.Variable(p)\n",
    "obj1 = cvx.Minimize(cvx.sum_squares(X@b1 - y))\n",
    "c1 = [cvx.norm(b1, 1) <= t]\n",
    "prob1 = cvx.Problem(obj1, c1)\n",
    "start_time = time.time()\n",
    "prob1.solve()\n",
    "end_time = time.time()\n",
    "e_la1 = np.sum((X@b1.value - z)**2)\n",
    "print(\"In sample error for the Lasso regression (formulation contrainte): {:0.4f}\".format(e_la1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa26e4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avec t = 7.2826, on retrouve bien la même erreur que la formulation precedente avec le lambda\n",
      "Temps prit pour résoudre le problème Lasso: 0.5309 secondes\n",
      "Le nombre de coefficients non nuls est de 24\n",
      "Les coefficients non nuls sont aux indices: [ 18  22  42  43  47  55  70  82  93 118 178 190 191 261 275 291 302 309\n",
      " 310 329 389 390 393 397]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Avec t = {t:0.4f}, on retrouve bien la même erreur que la formulation precedente avec le lambda\")\n",
    "print(\"Temps prit pour résoudre le problème Lasso: {:0.4f} secondes\".format(end_time - start_time))\n",
    "print(\"Le nombre de coefficients non nuls est de {:d}\".format(np.sum(np.abs(b1.value)>1e-4)))\n",
    "print(f\"Les coefficients non nuls sont aux indices: {np.where(np.abs(b1.value)>1e-4)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70682e34",
   "metadata": {},
   "source": [
    "#### c) Écrire un programme CVX résolvant la formulation suivant de Lasso, avec une valeur <br> de ε permettant d’obtenir les mêmes résultats que le problème précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "377ff3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In sample error for the Lasso regression (formulation contrainte): 0.3322\n",
      "Temps de calcul pour la formulation pénalisée: 0.5832 secondes\n"
     ]
    }
   ],
   "source": [
    "ε = np.sum((X@b1.value - y)**2)\n",
    "b2 = cvx.Variable(p)\n",
    "obj2 = cvx.Minimize(cvx.norm(b2, 1))\n",
    "c2 = [cvx.sum_squares(X@b2 - y) <= ε]\n",
    "prob2 = cvx.Problem(obj2, c2)\n",
    "start_time = time.time()\n",
    "prob2.solve()\n",
    "end_time = time.time()\n",
    "e_la2 = np.sum((X@b2.value - z)**2)\n",
    "print(\"In sample error for the Lasso regression (formulation contrainte): {:0.4f}\".format(e_la2))\n",
    "print(f\"Temps de calcul pour la formulation pénalisée: {end_time - start_time:0.4f} secondes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76916013",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANALYSE DES RÉSULTATS\n",
    "# avec lambda = 1e-3*n, t, ε, on retrouve bien les mêmes erreurs pour les 3 formulations du Lasso.\n",
    "# Le temps de calcul est plus élevé pour la formulation contrainte avec ε.\n",
    "# Le Lasso est bien meilleur que les moindres carrés en terme d'erreur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6c76abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In sample error for the LS regression: 2.7231\n",
      "In sample error for the ridge regression: 2.3094\n"
     ]
    }
   ],
   "source": [
    "λ = 1 # le paramètre de régularisation\n",
    "b_ridge = cvx.Variable(p) # le vecteur des coefficients, il sert de variable d'optimisation\n",
    "obj = cvx.Minimize(0.5*cvx.sum_squares(X@b_ridge - y) + λ*cvx.sum_squares(b_ridge))\n",
    "prob = cvx.Problem(obj) # le problème d'optimisation\n",
    "start_time = time.time()\n",
    "prob.solve(solver=cvx.SCS, eps=1e-5) # résolution du problème avec le solveur SCS\n",
    "end_time = time.time()\n",
    "\n",
    "# b) Vérifiez que la solution obtenue est meilleure que celle des moindres carrés\n",
    "b_ls = np.linalg.solve(X.T@X,X.T@y) # recalcul des coefficients de la régression des moindres carrés\n",
    "e_ls = np.sum((X@b_ls-z)**2) \n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format}) # pour afficher les nombres avec 3 décimales\n",
    "print(\"In sample error for the LS regression: {:0.4f}\".format(e_ls))\n",
    "\n",
    "e_ridge = np.sum((X@b_ridge.value-z)**2)\n",
    "print(\"In sample error for the ridge regression: {:0.4f}\".format(e_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7e72fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(6.536919119380083)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = (X.T @ X) @ b.value - X.T @ y + 2 * λ * b.value\n",
    "np.linalg.norm(grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f5a9290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge regression: 2.3094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(3.665773238953586e-15)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_ridge(X, y, lam):\n",
    "    n, p = X.shape\n",
    "    A = X.T @ X + 2.0 * lam * np.eye(p) \n",
    "    rhs = X.T @ y                         \n",
    "    b = np.linalg.solve(A, rhs)\n",
    "    return b\n",
    "\n",
    "b_ridge = my_ridge(X, y, λ)\n",
    "e_ridge = np.sum((X @ b_ridge - z) ** 2)\n",
    "print(\"ridge regression: {:0.4f}\".format(e_ridge))\n",
    "grad = (X.T @ X) @ b_ridge - X.T @ y + 2 * λ * b_ridge\n",
    "np.linalg.norm(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6d1007db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def check_KKT(X, y, beta, t, tol=1e-6):\n",
    "#     # stationnarité : X^T(Xβ - y) + μβ = 0\n",
    "#     residual = X @ beta - y\n",
    "#     grad = X.T @ residual   # gradient OLS\n",
    "#     norm_sq = 0.5 * np.linalg.norm(beta)**2\n",
    "\n",
    "#     # Calcul du multiplicateur de Lagrange μ\n",
    "#     if np.allclose(norm_sq, t, atol=tol):\n",
    "#         # contrainte active → μ > 0\n",
    "#         mu = - (grad @ beta) / (np.linalg.norm(beta)**2 + 1e-12)\n",
    "#     else:\n",
    "#         # contrainte inactive → μ = 0\n",
    "#         mu = 0.0\n",
    "\n",
    "#     # Vérification des KKT\n",
    "#     stationarity = np.linalg.norm(grad + mu * beta)\n",
    "#     complementarity = mu * (norm_sq - t)\n",
    "\n",
    "#     return {\n",
    "#         \"norm_sq\": norm_sq,\n",
    "#         \"mu\": mu,\n",
    "#         \"stationarity\": stationarity,\n",
    "#         \"complementarity\": complementarity\n",
    "#     }\n",
    "\n",
    "\n",
    "# check = check_KKT(X, y, b_ridge, t=1e6)\n",
    "# print(check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2342cfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norme de la solution: 5.0140 (doit être <= 5.0140)\n",
      "respecte la contrainte: True\n",
      "||stationnarité||: 1.7448e+00\n",
      "max violation stationnarité: 7.6730e-01\n",
      "complementarité: [ 1.302]\n",
      "lambda_hat >= 0 ? [ True]\n"
     ]
    }
   ],
   "source": [
    "t = 2.507\n",
    "\n",
    "b_ridge = cvx.Variable(p)\n",
    "constraint = [0.5 * cvx.sum_squares(b_ridge) <= t]\n",
    "obj = cvx.Minimize(cvx.sum_squares(X @ b_ridge - y))\n",
    "prob = cvx.Problem(obj, constraints=constraint)\n",
    "prob.solve(solver=cvx.SCS, eps=1e-5)\n",
    "\n",
    "beta_hat = b_ridge.value\n",
    "print(f\"norme de la solution: {np.linalg.norm(beta_hat)**2:0.4f} (doit être <= {2*t:0.4f})\")\n",
    "print(f\"respecte la contrainte: {0.5 * np.linalg.norm(beta_hat)**2 <= t + 1e-4}\")\n",
    "\n",
    "## KKT\n",
    "grad = X.T @ (X @ beta_hat - y)\n",
    "\n",
    "# multiplicateur de Lagrange μ associé à la contrainte\n",
    "lambda_hat = constraint[0].dual_value\n",
    "\n",
    "# stationnarité\n",
    "stationnarity = grad + 2 * lambda_hat * beta_hat\n",
    "print(f\"||stationnarité||: {np.linalg.norm(stationnarity):.4e}\")\n",
    "print(f\"max violation stationnarité: {np.max(np.abs(stationnarity)):.4e}\")\n",
    "\n",
    "print(f\"complementarité: {lambda_hat * (np.linalg.norm(beta_hat)**2 - t)}\")\n",
    "print(\"lambda_hat >= 0 ?\", lambda_hat >= -1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd76d62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
