{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc5c99e",
   "metadata": {},
   "source": [
    "TP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2211e588",
   "metadata": {},
   "source": [
    "### 1. Génération des données du problème. <br>\n",
    "#### a) Générez les données du problème"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24bebc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: [309 390  55  82 329]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxpy as cvx\n",
    "import time\n",
    "n = 200 # number of examples (you can try with n = 1000 and n = 5000)\n",
    "p = 2*n # dimensionality of the problem\n",
    "k = 5 # number of active variables\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(n,p) # creating features and normalizing them\n",
    "X = (X - np.mean(X,axis = 0))/np.std(X,axis = 0)\n",
    "t = np.arange(0,p)/(p-1); # bulding the variance matrix !\n",
    "S = np.zeros((p,p))\n",
    "nn = 0.00001\n",
    "for i in range(p):\n",
    "    S[i,:] = np.exp(-(t-t[i])**2/nn);\n",
    "X = X@(S**.5)\n",
    "X = X/np.linalg.norm(X,axis=0)\n",
    "ind = np.random.choice(p, k, replace=False) # generating optimal weights\n",
    "print(f\"index: {ind}\")\n",
    "weights = np.random.randn(k)\n",
    "weights += 0.1+np.sign(weights) # to get large enough weight\n",
    "wopt = np.zeros(p)\n",
    "wopt[ind] = weights\n",
    "rsnr = 2 # generating output by X@w + noise\n",
    "z = X[:,ind]@weights\n",
    "stdnoise = np.std(z)/rsnr\n",
    "y = z + stdnoise*np.random.randn(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd871cb",
   "metadata": {},
   "source": [
    "#### b) Vérifiez que les données ont bien les propriétés attendues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ca952c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de X: (200, 400)\n",
      "Taille de Y: (200,)\n",
      "Taille de S: (400, 400)\n",
      "weights: [-1.12180992  1.31787313 -0.92626818  2.65803182 -1.51223091]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Taille de X: {X.shape}\")\n",
    "print(f\"Taille de Y: {y.shape}\")\n",
    "print(f\"Taille de S: {S.shape}\")\n",
    "print(f\"weights: {weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f54845e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error for the LS regression: 2.7231\n"
     ]
    }
   ],
   "source": [
    "b_ls = np.linalg.solve(X.T@X,X.T@y)\n",
    "e_ls = np.sum((X@b_ls-z)**2)\n",
    "print(\"Test error for the LS regression: {:0.4f}\".format(e_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6c76abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In sample error for the LS regression: 2.7231\n",
      "In sample error for the ridge regression: 2.3094\n"
     ]
    }
   ],
   "source": [
    "λ = 1 # le paramètre de régularisation\n",
    "b_ridge = cvx.Variable(p) # le vecteur des coefficients, il sert de variable d'optimisation\n",
    "obj = cvx.Minimize(0.5*cvx.sum_squares(X@b_ridge - y) + λ*cvx.sum_squares(b_ridge))\n",
    "prob = cvx.Problem(obj) # le problème d'optimisation\n",
    "start_time = time.time()\n",
    "prob.solve(solver=cvx.SCS, eps=1e-5) # résolution du problème avec le solveur SCS\n",
    "end_time = time.time()\n",
    "\n",
    "# b) Vérifiez que la solution obtenue est meilleure que celle des moindres carrés\n",
    "b_ls = np.linalg.solve(X.T@X,X.T@y) # recalcul des coefficients de la régression des moindres carrés\n",
    "e_ls = np.sum((X@b_ls-z)**2) \n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format}) # pour afficher les nombres avec 3 décimales\n",
    "print(\"In sample error for the LS regression: {:0.4f}\".format(e_ls))\n",
    "\n",
    "e_ridge = np.sum((X@b_ridge.value-z)**2)\n",
    "print(\"In sample error for the ridge regression: {:0.4f}\".format(e_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7e72fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.2185169067776875e-07)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = (X.T @ X) @ b_ridge.value - X.T @ y + 2 * λ * b_ridge.value\n",
    "np.linalg.norm(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f5a9290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge regression: 2.3094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(3.665773238953586e-15)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_ridge(X, y, lam):\n",
    "    n, p = X.shape\n",
    "    A = X.T @ X + 2.0 * lam * np.eye(p) \n",
    "    rhs = X.T @ y                         \n",
    "    b = np.linalg.solve(A, rhs)\n",
    "    return b\n",
    "\n",
    "b_ridge = my_ridge(X, y, λ)\n",
    "e_ridge = np.sum((X @ b_ridge - z) ** 2)\n",
    "print(\"ridge regression: {:0.4f}\".format(e_ridge))\n",
    "grad = (X.T @ X) @ b_ridge - X.T @ y + 2 * λ * b_ridge\n",
    "np.linalg.norm(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2342cfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norme de la solution: 5.0140 (doit être <= 5.0140)\n",
      "respecte la contrainte: True\n",
      "||stationnarité||: 1.7448e+00\n",
      "max violation stationnarité: 7.6730e-01\n",
      "complementarité: [ 1.302]\n",
      "lambda_hat >= 0 ? [ True]\n"
     ]
    }
   ],
   "source": [
    "t = 2.507\n",
    "\n",
    "b_ridge = cvx.Variable(p)\n",
    "constraint = [0.5 * cvx.sum_squares(b_ridge) <= t]\n",
    "obj = cvx.Minimize(cvx.sum_squares(X @ b_ridge - y))\n",
    "prob = cvx.Problem(obj, constraints=constraint)\n",
    "prob.solve(solver=cvx.SCS, eps=1e-5)\n",
    "\n",
    "beta_hat = b_ridge.value\n",
    "print(f\"norme de la solution: {np.linalg.norm(beta_hat)**2:0.4f} (doit être <= {2*t:0.4f})\")\n",
    "print(f\"respecte la contrainte: {0.5 * np.linalg.norm(beta_hat)**2 <= t + 1e-4}\")\n",
    "\n",
    "## KKT\n",
    "grad = X.T @ (X @ beta_hat - y)\n",
    "\n",
    "# multiplicateur de Lagrange μ associé à la contrainte\n",
    "lambda_hat = constraint[0].dual_value\n",
    "\n",
    "# stationnarité\n",
    "stationnarity = grad + 2 * lambda_hat * beta_hat\n",
    "print(f\"||stationnarité||: {np.linalg.norm(stationnarity):.4e}\")\n",
    "print(f\"max violation stationnarité: {np.max(np.abs(stationnarity)):.4e}\")\n",
    "\n",
    "print(f\"complementarité: {lambda_hat * (np.linalg.norm(beta_hat)**2 - t)}\")\n",
    "print(\"lambda_hat >= 0 ?\", lambda_hat >= -1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bd76d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value for the Lasso problem: 2.7094\n",
      "||optimality||: 3.9193e+00\n"
     ]
    }
   ],
   "source": [
    "# Sous gradient et sous différentiabilité\n",
    "# On considère le problème de Lasso\n",
    "lambda_lasso = 0.001*n\n",
    "b_lasso = cvx.Variable(p)\n",
    "obj = cvx.Minimize(0.5 * cvx.sum_squares(X @ b_lasso - y) + lambda_lasso * cvx.norm1(b_lasso))\n",
    "prob = cvx.Problem(obj)\n",
    "\n",
    "prob.solve(solver=cvx.SCS, eps=1e-5)\n",
    "print(\"Optimal value for the Lasso problem: {:.4f}\".format(prob.value))\n",
    "\n",
    "# A quelles conditions la solution est-elle optimale ?\n",
    "beta_lasso = b_lasso.value\n",
    "grad = X.T @ (X @ beta_lasso - y)\n",
    "# On calcule un sous gradient du terme de régularisation\n",
    "subgrad = lambda_lasso * np.sign(beta_lasso)\n",
    "subgrad[beta_lasso == 0] = lambda_lasso * np.clip(grad[beta_lasso == 0]/lambda_lasso, -1, 1)\n",
    "# Condition d'optimalité : 0 ∈ ∇f(β) + λ ∂||β||_1\n",
    "optimality = grad + subgrad\n",
    "print(\"||optimality||: {:.4e}\".format(np.linalg.norm(optimality)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
